Decision trees are a popular class of supervised learning algorithms used for both classification and regression tasks. They work by recursively partitioning the input space into regions that are as homogeneous as possible with respect to the target variable. Let's dive deep into decision trees, covering their parameters, mathematical foundation, and provide an example using an imbalanced dataset.

### Mathematical Foundation of Decision Trees:

1. **Entropy (Information Gain)**: Entropy is a measure of impurity or randomness in a set of examples. For a binary classification problem with classes \( p \) and \( n \), the entropy \( H(S) \) of a set \( S \) is calculated as:
\[ H(S) = -p \log_2(p) - n \log_2(n) \]

2. **Information Gain**: Information Gain measures the reduction in entropy after splitting a set based on a feature. It helps in deciding the best feature to split on. For a feature \( A \) and a set \( S \), Information Gain \( IG(S, A) \) is calculated as:
\[ IG(S, A) = H(S) - \sum_{v \in values(A)} \frac{|S_v|}{|S|} H(S_v) \]
where \( values(A) \) are the possible values of feature \( A \), \( S_v \) is the subset of \( S \) where feature \( A \) has value \( v \), and \( |S_v| \) is the number of examples in \( S_v \).

3. **Gini Impurity**: Gini Impurity measures the probability of misclassifying an example at random. For a set \( S \), Gini Impurity \( G(S) \) is calculated as:
\[ G(S) = 1 - \sum_{i=1}^{n} p_i^2 \]
where \( p_i \) is the probability of an example belonging to class \( i \).

### Parameters of Decision Trees:

1. **Criterion**: The criterion parameter (`criterion`) determines the function to measure the quality of a split. It can be either 'gini' for Gini Impurity or 'entropy' for Information Gain.

2. **Splitting Strategy**: The splitting strategy (`splitter`) determines how the tree chooses the best split at each node. It can be 'best' to choose the best split or 'random' to choose the best random split.

3. **Max Depth**: The max depth parameter (`max_depth`) limits the maximum depth of the tree. It helps control overfitting.

4. **Min Samples Split**: The min samples split parameter (`min_samples_split`) sets the minimum number of samples required to split an internal node.

5. **Class Weights**: For imbalanced datasets, the class weights parameter (`class_weight`) assigns different weights to classes, giving more importance to minority classes.

### Example using an Imbalanced Dataset:

Let's consider an example using the Breast Cancer Wisconsin (Diagnostic) dataset, where the 'malignant' class is the minority class (imbalanced dataset).

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# Load the dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree classifier with various parameters
dt_classifier = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=3, min_samples_split=10, class_weight='balanced')

# Train the classifier
dt_classifier.fit(X_train, y_train)

# Make predictions
y_pred = dt_classifier.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred, target_names=data.target_names))
```

In this example:

- We load the Breast Cancer Wisconsin dataset and split it into training and testing sets.
- We create a Decision Tree classifier (`DecisionTreeClassifier`) with the following parameters:
  - `criterion='gini'`: Using Gini Impurity as the splitting criterion.
  - `splitter='best'`: Choosing the best split at each node.
  - `max_depth=3`: Limiting the maximum depth of the tree to control overfitting.
  - `min_samples_split=10`: Setting the minimum number of samples required to split an internal node.
  - `class_weight='balanced'`: Assigning weights inversely proportional to class frequencies to handle class imbalance.

After training and making predictions, we print the classification report to evaluate the model's performance, considering precision, recall, F1-score, and support for each class ('malignant' and 'benign'). Adjusting these parameters allows us to optimize the Decision Tree model's performance for an imbalanced dataset while controlling overfitting.
